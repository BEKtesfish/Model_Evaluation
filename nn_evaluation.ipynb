{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65e6cc34-e37b-4b14-a5a4-eec92cacb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for array computations and loading data\n",
    "import numpy as np\n",
    "\n",
    "# for ploting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for building linear regression models and preparing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# for building and training neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# suppress warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cdf42-2fc8-42d0-bff2-ab27e2d6fcc0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cb6fac93-bbce-4918-adb1-d358ca374708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the input(x): (50, 1)\n",
      "The shape of the target value(y): (50, 1)\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "data = np.loadtxt('/Users/berekettesfaye/Desktop/ML_projects/coursera_evaluation/data_w3_ex1.csv',delimiter = ',')\n",
    "\n",
    "# split the inputs and  outputs into separate arrays\n",
    "x = data[:, 0]\n",
    "y = data[:, 1]\n",
    "\n",
    "# convert 1-D arrays into 2-S because the commandes later will requaire it\n",
    "x = np.expand_dims(x, axis=1)\n",
    "y = np.expand_dims(y, axis=1)\n",
    "\n",
    "print(f\"The shape of the input(x): {x.shape}\")\n",
    "print(f\"The shape of the target value(y): {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a95406-58c1-48fd-8177-815f1b9f89a9",
   "metadata": {},
   "source": [
    "\n",
    "----Split the dataset into training, cross validation, and test sets----\n",
    "\n",
    "training set - used to train the model\n",
    "\n",
    "cross validation set (also called validation, development, or dev set) - used to evaluate the different model configurations you are choosing from. For example, you can use this to make a decision on what polynomial features to add to your dataset.\n",
    "\n",
    "test set - used to give a fair estimate of your chosen model's performance against new examples. This should not be used to make decisions while you are still developing the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79dfa223-9b16-448a-a362-d39a4d782452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (30, 1)\n",
      "the shape of the training set (target) is: (30, 1)\n",
      "\n",
      "the shape of the cross validation set (input) is: (10, 1)\n",
      "the shape of the cross validation set (target) is: (10, 1)\n",
      "\n",
      "the shape of the test set (input) is: (10, 1)\n",
      "the shape of the test set (target) is: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# get the 60% of the dataset as the training set. and save the rest 40% in x_,y_\n",
    "x_train,x_,y_train,y_ = train_test_split(x,y,test_size=0.40, random_state=1)\n",
    "\n",
    "#use  x_ and y_ to creat cross validation and test set\n",
    "x_cv,x_test, y_cv, y_test = train_test_split(x_,y_,test_size= 0.50, random_state=1)\n",
    "\n",
    "# delete x_, y_ because we are not gonna use it\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226803e-5da3-48a6-b26a-7a4a86866284",
   "metadata": {},
   "source": [
    "- i will perform feature scaleing on the training dataset to help my model to converge faster\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2356e39b-ecfb-4f58-8a54-0e8ffe8c5533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed mean of the training set: 2504.06\n",
      "Computed standard deviation of the training set: 574.85\n"
     ]
    }
   ],
   "source": [
    "# Intialize the class\n",
    "scaler_neural = StandardScaler()\n",
    "\n",
    "# Computes the mean and standard deviation of the training set then transform it\n",
    "x_train_scaled = scaler_neural.fit_transform(x_train)\n",
    "x_cv_scaled = scaler_neural.transform(x_cv)\n",
    "x_test_scaled = scaler_neural.transform(x_test)\n",
    "\n",
    "print(f\"Computed mean of the training set: {scaler_neural.mean_.squeeze():.2f}\")\n",
    "print(f\"Computed standard deviation of the training set: {scaler_neural.scale_.squeeze():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9177b-51a7-4ba0-9034-821187354a5f",
   "metadata": {},
   "source": [
    "* i will be evaluating between three neural network architectures\n",
    "  1. model_1 has 1 input layer with 25 units and activation is relu, 1 hidden layer 15 units activation is relu and 1 output layer with 1 unit and activation is linear\n",
    "  2. model_2 has 1 input layer with 20 units activation is relu, 3 hidden layer the first two has 12 units and the last one has 20 units and activation is relu and it has one output layer it has 1 unit  activation is linear\n",
    "  3. model_3 has 1 input layer with 32 units and activation is relu and it has 4 hidden layer (1 h_layer 16 units, 2 h_layer 8 units, 3 h_layer 4 units, 4 h_layer 12 units) and activation is relu for all of them and it has 1 output layer with one unit and activation is linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ba4baa3-4486-482f-be4a-699078ce902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models():\n",
    "    tf.random.set_seed(20)\n",
    "\n",
    "    model_1 = Sequential(\n",
    "        [\n",
    "            Dense(25, activation = 'relu'),\n",
    "            Dense(15, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name = 'model_1'\n",
    "    )\n",
    "\n",
    "    tf.random.set_seed(20)\n",
    "    model_2 = Sequential(\n",
    "        [\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name = 'model_2'\n",
    "    )\n",
    "    \n",
    "    tf.random.set_seed(20)\n",
    "    model_3 = Sequential(\n",
    "        [\n",
    "            Dense(32, activation = 'relu'),\n",
    "            Dense(16, activation = 'relu'),\n",
    "            Dense(8, activation = 'relu'),\n",
    "            Dense(4, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name = 'model_3'\n",
    "    )\n",
    "\n",
    "    model_list = [model_1, model_2, model_3]\n",
    "\n",
    "    return model_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac847919-4e1c-4642-ac37-7ecd4ba5e875",
   "metadata": {},
   "source": [
    "* Build and train the models\n",
    "    -For each model, i will  record the training and cross validation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "228d1a9e-90dd-46c2-94b6-89d0bab74fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model_1...\n",
      "Done!\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Training model_2...\n",
      "Done!\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Training model_3...\n",
      "Done!\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Results: \n",
      "Model 1: Training MSE: 406.19, CV MSE: 551.78\n",
      "Model 2: Training MSE: 73.40, CV MSE: 112.28\n",
      "Model 3: Training MSE: 73.78, CV MSE: 112.96\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_mses = []\n",
    "nn_cv_mses = []\n",
    "\n",
    "# build the models\n",
    "nn_models = build_models()\n",
    "# Print summaries to ensure the models are built\n",
    "\n",
    "# loop over the models \n",
    "for model in nn_models:\n",
    "     \n",
    "    #setup the loss and optimizer\n",
    "    model.compile(\n",
    "        loss = 'mse',\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    )\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_train_scaled,y_train, \n",
    "        epochs=300,\n",
    "        verbose=0\n",
    "    )\n",
    "    print(\"Done!\\n\")\n",
    "  \n",
    "\n",
    "    # Recored the training MSE\n",
    "    yhat = model.predict(x_train_scaled)\n",
    "    train_mse = mean_squared_error(y_train,yhat) / 2\n",
    "    nn_train_mses.append(train_mse)\n",
    "\n",
    "    # Record the cross validation MSE\n",
    "    yhat = model.predict(x_cv_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv,yhat) / 2\n",
    "    nn_cv_mses.append(cv_mse)\n",
    "\n",
    "#print the results\n",
    "print(\"Results: \")\n",
    "for model_num in range(len(nn_train_mses)):\n",
    "    print(\n",
    "        f\"Model {model_num + 1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n",
    "        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b20a66-b9b5-46d2-9006-f1d7f3fc2ea7",
   "metadata": {},
   "source": [
    "* From the recorded errors , i will decide the best model for my application. and use it to estimate the error on the test set to know how well it generalizesn to new examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f939b5a3-5250-403e-b978-5e2f1bc884d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Selected Model: 2\n",
      "Training MSE: 73.40\n",
      "Cross Validation MSE: 112.28\n",
      "Test MSE: 131.57\n"
     ]
    }
   ],
   "source": [
    "# Select the model with the lowest CV MSE\n",
    "model_num = 2\n",
    "# compute the test MSE\n",
    "yhat = nn_models[model_num - 1 ].predict(x_test_scaled)\n",
    "test_mse = mean_squared_error(y_test,yhat)/2\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
